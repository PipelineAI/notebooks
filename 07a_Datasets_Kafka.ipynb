{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to https://community.pipeline.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model to PipelineAI\n",
    "\n",
    "You will need to fill in the unique values for the following:\n",
    "* `<YOUR_USER_ID>`  - 8 character id that uniquely identifies the PipelineAI user.  You will see the UserId in the upper right hand corner of the Settings tab after you login to [PipelineAI Community Edition](https://community.cloud.pipeline.ai)\n",
    "* `<UNIQUE_MODEL_NAME>` - User-defined model/project name that uniquely identifies a model/project within your account.\n",
    "* `<UNIQUE_TAG_NAME>` - User-defined tag that uniquely identifies the model tag/version within a model/project\n",
    "\n",
    "\n",
    "![user-id](https://pipeline.ai/assets/img/user-id.png)\n",
    "\n",
    "```\n",
    "pipeline resource-upload --host community.cloud.pipeline.ai --user-id <YOUR_USER_ID> --resource-type model --resource-subtype tensorflow  --name <UNIQUE_MODEL_NAME> --tag <UNIQUE_TAG_NAME> --path ./tensorflow/mnist-v1/model\n",
    "```\n",
    "\n",
    "Actions performed:\n",
    "* Compress resource source code into a tar archive.\n",
    "* Create required directories and generate deployment and service resource definitions.\n",
    "* Receive resource source code - or trained binary (ie. tensorflow SavedModel binary)\n",
    "  from client as a tar archive then uncompress and extract on the PipelineAI server.\n",
    "* Initialize training resource\n",
    "\n",
    "# Optimize and Deploy the Model\n",
    "You can optimize (select one or more chips and/or one or more runtimes) and deploy your model using the CLI or the [UI](https://community.cloud.pipeline.ai) (Choose either the CLI or UI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "from tensorflow.contrib.kafka.python.ops import kafka_dataset_ops\n",
    "from tensorflow.python.data.ops import iterator_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import errors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.platform import test\n",
    "import tensorflow as tf\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "x_train = np.random.rand(num_samples).astype(np.float32)\n",
    "print(x_train)\n",
    "\n",
    "noise = np.random.normal(scale=0.01, size=len(x_train))\n",
    "\n",
    "y_train = x_train * 0.1 + 0.3 + noise\n",
    "print(y_train)\n",
    "\n",
    "pylab.plot(x_train, y_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = np.random.rand(len(x_train)).astype(np.float32)\n",
    "print(x_test)\n",
    "\n",
    "noise = np.random.normal(scale=.01, size=len(x_train))\n",
    "\n",
    "y_test = x_test * 0.1 + 0.3 + noise\n",
    "print(y_test)\n",
    "\n",
    "pylab.plot(x_test, y_test, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = '83f05e58' + 'linear'\n",
    "\n",
    "p = Producer({'bootstrap.servers': 'kafka-cp-kafka-headless:9092'})\n",
    "\n",
    "for x, y in zip(x_train, y_train):\n",
    "    p.produce(topic_name, str((x, y)).encode('utf-8'))\n",
    "\n",
    "if (p.flush() == 0):\n",
    "    print('Success')\n",
    "else:\n",
    "    print('Failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSingleRecord(topic_name):\n",
    "    topics = array_ops.placeholder(dtypes.string, shape=[None])\n",
    "    num_epochs = array_ops.placeholder(dtypes.int64, shape=[])\n",
    "    batch_size = array_ops.placeholder(dtypes.int64, shape=[])\n",
    "\n",
    "    repeat_dataset = kafka_dataset_ops.KafkaDataset(\n",
    "        servers=\"kafka-cp-kafka-headless:9092\", \n",
    "        topics=[topic_name], \n",
    "        group=\"community\", \n",
    "        eof=False).repeat(num_epochs)\n",
    "    \n",
    "    batch_dataset = repeat_dataset.batch(batch_size)\n",
    "\n",
    "    iterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)\n",
    "    init_op = iterator.make_initializer(repeat_dataset)\n",
    "    get_next = iterator.get_next()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Read from topic\n",
    "        sess.run(\n",
    "            init_op,\n",
    "            feed_dict={\n",
    "              topics: [topic_name],\n",
    "              num_epochs: 1,\n",
    "              batch_size: 5\n",
    "            })\n",
    "        try:\n",
    "            print(sess.run(get_next).decode('utf-8'))\n",
    "        except:\n",
    "            print('No more messages to process.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = getSingleRecord(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMultipleRecords(topic_name):\n",
    "    topics = array_ops.placeholder(dtypes.string, shape=[None])\n",
    "    num_epochs = array_ops.placeholder(dtypes.int64, shape=[])\n",
    "    batch_size = array_ops.placeholder(dtypes.int64, shape=[])\n",
    "\n",
    "    repeat_dataset = kafka_dataset_ops.KafkaDataset(\n",
    "      servers=\"kafka-cp-kafka-headless:9092\", \n",
    "      topics=[topic_name], \n",
    "      group=\"community\", \n",
    "      eof=False).repeat(num_epochs)\n",
    "\n",
    "    batch_dataset = repeat_dataset.batch(batch_size)\n",
    "\n",
    "    iterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)\n",
    "    init_op = iterator.make_initializer(repeat_dataset)\n",
    "    get_next = iterator.get_next()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Batched and repeated iteration through the stream.\n",
    "        init_batch_op = iterator.make_initializer(batch_dataset)\n",
    "        sess.run(\n",
    "          init_batch_op,\n",
    "            feed_dict={\n",
    "            topics: [topic_name],\n",
    "            num_epochs: 20,\n",
    "            batch_size: 5\n",
    "        })\n",
    "\n",
    "        more_records = True\n",
    "        while more_records:\n",
    "           try:\n",
    "               print(sess.run(get_next))\n",
    "           except e:\n",
    "               print('No more messages to process.')\n",
    "               more_records = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = getMultipleRecords(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
